{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the required libraries, lots of these are required for the LLMs we utilize for three criteria. \n",
    "import spacy\n",
    "import pandas as pd\n",
    "from transformers import pipeline, AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from lexicalrichness import LexicalRichness\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import openai\n",
    "openai.api_key = 'REPLACE_WITH_YOUR_KEY'\n",
    "model_engine = \"text-davinci-003\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our class used to represent multiple-choice questions\n",
    "nl = '\\n'\n",
    "class MultipleChoiceQuestion:\n",
    "    def __init__(self, stem, options, correct_option, qid = None, courseid = None, quality = None):\n",
    "        self.stem = stem\n",
    "        self.options = options\n",
    "        self.correct_option = correct_option\n",
    "        self.qid = qid\n",
    "        self.courseid = courseid\n",
    "        self.quality = quality\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"Question: {self.stem}\\n {nl.join(self.options)}\\nCorrect option: {self.correct_option}\\nQuality: {self.quality}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19 Item-Writing Flaw Criteria "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ambiguous or unclear information \n",
    "    Questions and all options should be written in clear, unambiguous language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Roberta model from: https://huggingface.co/cointegrated/roberta-large-cola-krishna2020\n",
    "cola = pipeline('text-classification', model='cointegrated/roberta-large-cola-krishna2020',truncation=True)\n",
    "\n",
    "def ambiguous_unclear_information(question):\n",
    "    output = cola(question.stem)\n",
    "    score = output[0]['score']\n",
    "    if score >= 0.7:\n",
    "        return True\n",
    "    else:\n",
    "        print('--- Question stem is unclear')\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implausible distractors\n",
    "    Make all distractors plausible as good items depend on having effective distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MiniLM from: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "#Uses NER, so if the score is too low, if they're matching entities (i.e. people) then we can ignore this case and say True\n",
    "def implausible_distractors(question):\n",
    "    correct = question.correct_option\n",
    "    options = question.options.copy()\n",
    "    options.remove(correct)\n",
    "\n",
    "    # Two lists of sentences\n",
    "    sentences1 = [correct, correct, correct]\n",
    "    sentences2 = options\n",
    "\n",
    "    #Compute embedding for both lists\n",
    "    embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "    embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "    #Compute cosine-similarities\n",
    "    cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "    #Output the pairs with their score\n",
    "    for i in range(len(sentences1)):\n",
    "        if cosine_scores[i][i] < 0.15: #Was .2\n",
    "            \n",
    "            #NER check here...\n",
    "            opt_entity = nlp(sentences2[i])\n",
    "            lemma_nouns_opt = get_lemma_nouns(sentences2[i])\n",
    "            \n",
    "            ans_entity = nlp(sentences1[i])\n",
    "            lemma_nouns_ans = get_lemma_nouns(sentences1[i])\n",
    "\n",
    "            #If the noun(s) in the answer choice can be tagged with an entity\n",
    "            if ans_entity.ents:\n",
    "                answer_entity = ans_entity.ents[0].label_\n",
    "            else:\n",
    "                answer_entity = None\n",
    "\n",
    "            if opt_entity.ents:\n",
    "                opt_entity = opt_entity.ents[0].label_\n",
    "            else:\n",
    "                opt_entity = None\n",
    "\n",
    "            if answer_entity and opt_entity and answer_entity in opt_entity:\n",
    "                #low score, but they are the same entity\n",
    "                return True\n",
    "            \n",
    "            if len(lemma_nouns_ans) == 0 and len(lemma_nouns_opt) == 0:\n",
    "                #Couldn't find the noun nor the entity? Unable to parse effectively to make a judgement.\n",
    "                return True\n",
    "            \n",
    "            #If the option in this case is none/all of the above, it won't be similar, so ignore this criteria\n",
    "            if not all_of_the_above(question) or not none_of_the_above(question):\n",
    "                return True\n",
    "            \n",
    "            print(\"--- Distractor not similar enough: {} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))\n",
    "            return False\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### None of the above\n",
    "    Avoid none of the above as it only really measures students ability to detect incorrect answers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def none_of_the_above(question):\n",
    "    for opt in question.options:\n",
    "\n",
    "        cleaned_opt = opt.strip().lower()\n",
    "        if 'none of the above' in cleaned_opt or ('none' in cleaned_opt and 'above' in cleaned_opt) or cleaned_opt == 'neither' or cleaned_opt == \"none\" or 'none' in question.options[3]:\n",
    "            print('--- None of the above')\n",
    "            return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longest option correct\n",
    "    Often the correct option is longer and includes more detailed information, which clues students to this option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the correct answer is noticably longer (20% or more) than the second longest answer, flag it.\n",
    "\n",
    "def longest_answer_correct(question):\n",
    "    correct = question.correct_option\n",
    "    options = question.options.copy()\n",
    "    options.remove(correct)\n",
    "    \n",
    "    longest_option = 0\n",
    "    for opt in options:\n",
    "        if len(opt) >= longest_option:\n",
    "            longest_option = len(opt)\n",
    "        \n",
    "    #If the longest option is only by 20% or it's a single word/number, then this passes\n",
    "    if longest_option >= len(correct) * 0.8 or len(correct.split()) == 1:\n",
    "        return True\n",
    "    \n",
    "    print('--- longest option is correct')\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gratuitous information\n",
    "    Avoid unnecessary information in the stem that is not required to answer the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gratuitous_information_in_stem(question):  \n",
    "    #How effective are lexical richness measures for differentiations of vocabulary proficiency? A comprehensive examination with clustering analysis\n",
    "    #From: https://github.com/LSYS/LexicalRichness\n",
    "    stem = LexicalRichness(question.stem)\n",
    "    \n",
    "    if stem.cttr > 4.5:\n",
    "        print(\"--- CTTR above 4.5, text is too complex and extraneous: \", stem.cttr)\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True/False question\n",
    "    The options should not be a series of true/false statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question should not be a series of true/false statements, so we can look for \"which\" and \"true\" or \"false\" in the stem\n",
    "\n",
    "def true_or_false(question):\n",
    "    correct = question.correct_option\n",
    "    options = question.options.copy()\n",
    "    options.remove(correct)\n",
    "    \n",
    "    #Check for neatively worded stem too.\n",
    "    for sent in question.stem.split('.'):\n",
    "        sent = sent.lower()\n",
    "        if 'which' in sent and ('false' in sent or 'true' in sent):\n",
    "            print('--- true/false or yes/no answer choice')\n",
    "            return False    \n",
    "    \n",
    "    for opt in options:\n",
    "        if opt.strip().lower() == 'true' or opt.strip().lower() == 'false' or opt.strip().lower() == 'yes'or opt.strip().lower() == 'no':\n",
    "            print('--- true/false or yes/no answer choice')\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence cues\n",
    "    Avoid convergence cues in options where there are different combinations of multiple components to the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for synonyms, because they'll know it's the word they've most recently come across in the text\n",
    "#The correct option is likely to be used more (when in pairs, etc.) --> k-type (super similar by description)\n",
    "\n",
    "def avoid_convergence_cues(question):\n",
    "    #So here we check for synonyms used in the words, in case they get lazy with distractors\n",
    "    options = question.options.copy()\n",
    "    options.remove(question.correct_option)\n",
    "    if len(options) < 3:\n",
    "        return True\n",
    "    \n",
    "    lemma_nouns_answ = get_lemma_nouns(question.correct_option)\n",
    "    lemma_nouns_options = [get_lemma_nouns(options[0]), get_lemma_nouns(options[1]), get_lemma_nouns(options[2])]\n",
    "    \n",
    "    #so we want this code, but like, for synonyms\n",
    "    synonyms = []\n",
    "    for noun in lemma_nouns_answ:\n",
    "        for syn in wn.synsets(noun):\n",
    "            for l in syn.lemmas():\n",
    "                synonyms.append(l.name().lower().replace('_', ' '))\n",
    "    \n",
    "    for opt in lemma_nouns_options:\n",
    "        repeating_nouns_synonyms = list(set(synonyms).intersection(opt))\n",
    "        print(\"repeating_nouns_synonyms: \", repeating_nouns_synonyms)\n",
    "        if len(repeating_nouns_synonyms) > 0:           \n",
    "            \n",
    "            #if the repeat is not in every answer choice, flag it.\n",
    "            for rns in repeating_nouns_synonyms:           \n",
    "                \n",
    "                flag = True\n",
    "                for value in lemma_nouns_options:\n",
    "                    if rns not in value:\n",
    "                        print('--- we have a synonym of the answer being used in other answer choices, but not all of them: ', rns)\n",
    "                        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logical cues\n",
    "    Avoid clues in the stem and the correct option that can help the test-wise student to identify the correct option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An example of a logical cue is asking students to select the most appropriate pharmaceutical intervention for a problem and\n",
    "#only having one or two options which\n",
    "#Using NER, if the question asks for a <certain type of noun, like a <person> then the options should all be <people> too.\n",
    "\n",
    "def avoid_logical_cues(question):\n",
    "    options = question.options.copy()\n",
    "    options.remove(question.correct_option)\n",
    "    if len(options) < 3:\n",
    "        return True\n",
    "    \n",
    "    lemma_nouns_answ = get_lemma_nouns(question.correct_option)\n",
    "    lemma_nouns_options = [get_lemma_nouns(options[0]), get_lemma_nouns(options[1]), get_lemma_nouns(options[2])]\n",
    "    \n",
    "    entities_in_options = []\n",
    "    for opt in lemma_nouns_options:\n",
    "        for val in opt:\n",
    "            doc = nlp(val)\n",
    "            if doc.ents:\n",
    "                entities_in_options.append(doc.ents[0].label_)\n",
    "    \n",
    "    entities_in_answer = []\n",
    "  \n",
    "    for val in lemma_nouns_answ:\n",
    "        doc = nlp(val)\n",
    "        \n",
    "        #If the noun(s) in the answer choice can be tagged with an entity\n",
    "        if doc.ents:\n",
    "            answer_entity = doc.ents[0].label_\n",
    "            if answer_entity not in entities_in_options:\n",
    "                print('--- The answer entity is not found in any other options: ', answer_entity)\n",
    "                return False \n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All of the above\n",
    "    Avoid all of the above options as students can guess correct responses based on partial information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_of_the_above(question):\n",
    "    for opt in question.options:\n",
    "        if 'all of the above' in opt or ('all' in opt and 'above' in opt) or ('all if the' in opt):\n",
    "            print('--- all of the above')\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill-in-blank\n",
    "    Avoid omitting words in the middle of the stem that students must insert from the options provided "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_in_the_blank(question):\n",
    "    if \"_\" in question.stem:\n",
    "        print('--- fill in the blank')\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absolute terms\n",
    "    Avoid the use of absolute terms (e.g. never, always, all) in the options as students are aware that they are almost \n",
    "    always false "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absolute_terms(question):\n",
    "    absolutes = [\"always\", \"never\", \"every\", \"none\", \"only\"]\n",
    "    for opt in question.options:\n",
    "        #Count all, but not in the case of \"all of the above\"\n",
    "        if any(word in opt for word in absolutes) or (\"all\" in opt and all_of_the_above(question)):\n",
    "            print('--- absolute word in question stem')\n",
    "            return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word repeats\n",
    "    Avoid similarly worded stems and correct responses or words repeated in the stem and correct response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the nouns in question.correct_option and question.stem --> stem them --> compare cosine similiary (usin sentence transformer)\n",
    "#Also check for the synonyms, compare them. However, if the word(s) are used in the other options, then it's fine.\n",
    "\n",
    "#Nouns: NN noun, singular ‘- desk’, NNS noun plural – ‘desks’, NNP proper noun, singular – ‘Harrison’, NNPS proper noun, plural – ‘Americans’ \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nouns = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "def word_repeats_in_stem_and_correct_answer(question):   \n",
    "    lemma_nouns_stem = get_lemma_nouns(question.stem)        \n",
    "    lemma_nouns_answ = get_lemma_nouns(question.correct_option)\n",
    "    \n",
    "    repeating_nouns = list(set(lemma_nouns_stem).intersection(lemma_nouns_answ))\n",
    "    \n",
    "    #Check for synonms in question stem w/ answer choice\n",
    "    synonyms = []\n",
    "    for noun in lemma_nouns_stem:\n",
    "        for syn in wn.synsets(noun):\n",
    "            for l in syn.lemmas():\n",
    "                synonyms.append(l.name().lower().replace('_', ' '))\n",
    "                \n",
    "    repeating_nouns_synonyms = list(set(synonyms).intersection(lemma_nouns_answ))\n",
    "    \n",
    "    #If we get a repeat, then it should also repeat in the other answer choices, not just the correct\n",
    "    if len(repeating_nouns) > 0 or len(repeating_nouns_synonyms) > 0:\n",
    "        options = question.options.copy()\n",
    "        options.remove(question.correct_option)\n",
    "        options_that_share_noun = 0\n",
    "        \n",
    "        for opt in options:\n",
    "            lemma_option = get_lemma_nouns(opt)\n",
    "            repeating_nouns_ans_opt = list(set(lemma_option).intersection(lemma_nouns_answ))\n",
    "            if len(repeating_nouns_ans_opt) > 0:\n",
    "                options_that_share_noun += 1\n",
    "        \n",
    "        #If the word is shared between all options, then it is fine\n",
    "        if options_that_share_noun == 3 or all_of_the_above(question) or none_of_the_above(question):\n",
    "            return True\n",
    "        else:\n",
    "            print('--- The noun is only shared in certain words')\n",
    "            return False\n",
    "        \n",
    "    #Check for word (adjective, noun, verb, adverb) that repeats just in stem and answer    \n",
    "    word_types = ['NN', 'NNS', 'NNP', 'NNPS', 'RB', 'RBR', 'RBS', 'VB', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "    stem_token = sent_tokenize(question.stem)\n",
    "    stem_words = []\n",
    "    for i in stem_token:\n",
    "        wordsList = nltk.word_tokenize(i)\n",
    "        wordsList = [w for w in wordsList if not w in stop_words]\n",
    "        tagged = nltk.pos_tag(wordsList)\n",
    "        for t in tagged:\n",
    "            if t[1] in word_types:\n",
    "                stem_words.append(t[0].lower())\n",
    "                \n",
    "    ans_token = sent_tokenize(question.correct_option)\n",
    "    ans_words = []\n",
    "    for i in ans_token:\n",
    "        wordsList = nltk.word_tokenize(i)\n",
    "        wordsList = [w for w in wordsList if not w in stop_words]\n",
    "        tagged = nltk.pos_tag(wordsList)\n",
    "        for t in tagged:\n",
    "            if t[1] in word_types:\n",
    "                ans_words.append(t[0].lower())\n",
    "    \n",
    "    if any(x in stem_words for x in ans_words):\n",
    "        return False\n",
    "\n",
    "    \n",
    "    return True\n",
    "\n",
    "def get_lemma_nouns(text):\n",
    "    all_nouns = []\n",
    "    tokenized = sent_tokenize(text)\n",
    "    \n",
    "    for i in tokenized:\n",
    "\n",
    "        # Word tokenizers is used to find the words and punctuation in a string\n",
    "        wordsList = nltk.word_tokenize(i)\n",
    "\n",
    "        # removing stop words from wordList\n",
    "        wordsList = [w for w in wordsList if not w in stop_words]\n",
    "\n",
    "        #  Using a Tagger. Which is part-of-speech tagger or POS-tagger.\n",
    "        tagged = nltk.pos_tag(wordsList)\n",
    "        \n",
    "        # Add any nouns to this list\n",
    "        for t in tagged:\n",
    "            if t[1] in nouns:\n",
    "                all_nouns.append(t[0].lower())\n",
    "    \n",
    "    lemmatized_nouns = []\n",
    "    for n in all_nouns:\n",
    "        lemmatized_word = lemmatizer.lemmatize(n, pos=\"n\")\n",
    "        lemmatized_nouns.append(lemmatized_word.lower())\n",
    "    \n",
    "    return lemmatized_nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unfocused stem\n",
    "    The stem should present a clear and focused question that can be understood and answered without looking at the options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfocused_stem(question):\n",
    "    contains_question = False\n",
    "    doc = nlp(question.stem)\n",
    "    for sent in doc.sents:\n",
    "        if is_question(sent.text.strip()):\n",
    "            contains_question = True\n",
    "            \n",
    "    if not contains_question:\n",
    "        print(\"--- Question stem does not contain a question\")\n",
    "        \n",
    "    return contains_question\n",
    "\n",
    "#From https://stackoverflow.com/questions/4083060/determine-if-a-sentence-is-an-inquiry\n",
    "def is_question(sent):\n",
    "    d = nlp(sent)\n",
    "    token = d[0] # gets the first token in a sentence\n",
    "    if token.pos_ == \"VERB\" and token.dep_ == \"ROOT\": # checks if the first token is a verb and root or not\n",
    "        return True\n",
    "    for token in d: # loops through the sentence and checks for WH tokens\n",
    "        if token.tag_ == \"WDT\" or token.tag_ == \"WP\" or token.tag_ == \"WP$\" or token.tag_ == \"WRB\" or token.text == '?':\n",
    "            return True\n",
    "    return  False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex or K-type\n",
    "    Avoid questions that have a range of correct responses, that ask students to select from a number of possible \n",
    "    combinations of the responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the answer options share the same words between one another and there are commas present then it's k type\n",
    "\n",
    "def complex_k_type(question):    \n",
    "    options = question.options.copy()\n",
    "    options.remove(question.correct_option)\n",
    "    if len(options) < 3:\n",
    "        return True \n",
    "    \n",
    "    #check if the options contain a comma\n",
    "    contain_a_comma = 0\n",
    "    for opt in options:\n",
    "        if ',' in opt:\n",
    "            contain_a_comma += 1\n",
    "    contain_a_comma = contain_a_comma == len(options)\n",
    "    \n",
    "    lemma_nouns_answ = get_lemma_nouns(question.correct_option)\n",
    "    lemma_nouns_options = [get_lemma_nouns(options[0]), get_lemma_nouns(options[1]), get_lemma_nouns(options[2])]\n",
    "    \n",
    "    options_that_share_noun = 0\n",
    "    for lno in lemma_nouns_options:   \n",
    "        repeating_nouns = list(set(lno).intersection(lemma_nouns_answ))\n",
    "        if (len(repeating_nouns) > 0) and (len(lno) > 0):\n",
    "            options_that_share_noun += 1\n",
    "    \n",
    "    #Options share a key word, there are multiple nouns in the options, and they have a comma\n",
    "    #suggesting it might be a k-type question\n",
    "    if options_that_share_noun > 0 and contain_a_comma:\n",
    "        print(\"--- This is a K-type question\")\n",
    "        return False\n",
    "    \n",
    "    #After removing any list notation in the answer choices, see if they contain the same words\n",
    "    cleaned_options = []\n",
    "    for opt in options:\n",
    "        cleaned_options.append(clean_string(opt))\n",
    "\n",
    "    options_set_list = [set(i.split()) for i in cleaned_options]\n",
    "    if options_set_list[0] == options_set_list[0] and options_set_list[0] == options_set_list[1] and options_set_list[0] == options_set_list[2]:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def clean_string(string):\n",
    "    # remove whitespace\n",
    "    cleaned_string = string.strip()\n",
    "    \n",
    "    # remove punctuation\n",
    "    cleaned_string = re.sub(r'[^\\w\\s]', '', cleaned_string)\n",
    "    \n",
    "    # remove list notation\n",
    "    cleaned_string = re.sub(r'\\b(i|ii|iii|iv|v|vi|vii|viii|ix|x|xi|xii|xiii|xiv|xv|xvi|xvii|xviii|xix|xx)\\b', '', cleaned_string)\n",
    "    cleaned_string = re.sub(r'\\b(A|B|C|D|E|F)\\b', '', cleaned_string)\n",
    "    print('cleaned_string: ', cleaned_string)\n",
    "    return cleaned_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammatical cues\n",
    "    All options should be grammatically consistent with the stem and should be parallel in style and form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If verb exists in answer choice, ensure it's the same tense as verb in other options\n",
    "#We want the stem to be the same, but as long as all the answers are the same, then it's fine, to avoid false positive.\n",
    "def grammatical_cues_in_stem(question):\n",
    "    stem_tense = get_verb_tense(question.stem)\n",
    "    answer_tense = get_verb_tense(question.correct_option)\n",
    "    \n",
    "    options = question.options.copy()\n",
    "    options.remove(question.correct_option)\n",
    "    for opt in options:\n",
    "        opt_tense = get_verb_tense(opt)\n",
    "        if opt_tense != 'none' and answer_tense is not opt_tense:\n",
    "            print(\"--- Verb tense doesn't align between answer other options\")\n",
    "            return False\n",
    "        \n",
    "    return True\n",
    "    \n",
    "def get_verb_tense(text):\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'VERB':\n",
    "            if token.tag_ in ['VBP', 'VBZ']:\n",
    "                return 'present'\n",
    "            elif token.tag_ in ['VBD', 'VBN']:\n",
    "                return 'past'\n",
    "            else:\n",
    "                return 'other'\n",
    "    return 'none'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lost sequence\n",
    "    All options should be arranged in chronological or numerical order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If answer choices are numeric, sort them, compare to current order\n",
    "def lost_sequence(question):\n",
    "    options = question.options.copy()\n",
    "    \n",
    "    opts = []\n",
    "    for opt in options:\n",
    "        opt = re.sub(r'[$%°FC,]', '', opt)\n",
    "        opts.append(opt)\n",
    "        try:\n",
    "            float(opt)\n",
    "        except ValueError:\n",
    "            return True\n",
    "        \n",
    "    float_options = [float(x) for x in opts]    \n",
    "    sorted_options = sorted(float_options)\n",
    "    if sorted_options == float_options:\n",
    "        #Numeric options are sorted\n",
    "        return True\n",
    "    else:\n",
    "        print('--- Options are numeric and not sorted')\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vague terms\n",
    "    Avoid the use of vague terms (e.g. frequently, occasionally) in the options as there is seldom agreement on their actual meaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vague_terms(question):\n",
    "    vagues = [\"often\", \"sometimes\", \"rarely\", \"typically\", \"usually\", \"normally\", \"generally\", \"nearly\", \"approximately\", \"more or less\", \"somewhat\"]\n",
    "    for opt in question.options:\n",
    "        if any(word in opt for word in vagues):\n",
    "            print('--- vague word in question stem')\n",
    "            return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More than one correct\n",
    "    In single best-answer form, questions should have 1, and only 1, best answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using GPT-4 for QA, the model can confirm if the correct answer is correct, however it is not that accurate.\n",
    "#In the future, utilize a better QA model as improvements are made.\n",
    "\n",
    "def more_than_one_correct(question):\n",
    "    if not question.options[2]:\n",
    "        question.options[2] = 'None'\n",
    "        \n",
    "    if not question.options[3]:\n",
    "        question.options[3] = 'None'\n",
    "    \n",
    "    # Define the prompt\n",
    "    prompt = \"\"\"\n",
    "    Answer the multiple-choice question below by responding with A, B, C, or D.\n",
    "    \n",
    "    {}\n",
    "\n",
    "    A) {}\n",
    "    B) {}\n",
    "    C) {}\n",
    "    D) {}\n",
    "    \"\"\".format(question.stem, question.options[0], question.options[1], question.options[2], question.options[3])\n",
    "\n",
    "    # Generate a response\n",
    "    while(done == False):\n",
    "        try:\n",
    "            o = openai.ChatCompletion.create(\n",
    "              model=\"gpt-4\",\n",
    "              messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "              ]\n",
    "             )\n",
    "            done = True \n",
    "        except:\n",
    "            #If there's an error from the API being down, wait 150 seconds before retrying\n",
    "            time.sleep(150)\n",
    "    completion = o\n",
    "    done = False\n",
    "\n",
    "    # Print the response\n",
    "    try:\n",
    "        cleaned_response = completion.choices[0].message.content.split(')')[0].strip()\n",
    "        if (cleaned_response == 'A'):\n",
    "            cleaned_response = question.options[0]\n",
    "        elif (cleaned_response == 'B'):\n",
    "            cleaned_response = question.options[1]\n",
    "        elif (cleaned_response == 'C'):\n",
    "            cleaned_response = question.options[2]\n",
    "        elif (cleaned_response == 'D'):\n",
    "            cleaned_response = question.options[3]\n",
    "    except: \n",
    "        print('error in GPT-4: ', completion)\n",
    "    \n",
    "    if cleaned_response == question.correct_option:\n",
    "        return True\n",
    "    else:\n",
    "        print('--- GPT-4 believes the answer is incorrect: ', cleaned_response, ' ', question.correct_option)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative worded\n",
    "    Negatively worded stems are less likely to measure important learning\n",
    "    outcomes and can confuse students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_worded_stem(question):\n",
    "    negatives = [\"no\", \"none\", \"never\", \"without\", \"exclude\", \"avoid\", \"deny\", \"refuse\", \"oppose\", \"dispute\"]\n",
    "    for opt in question.stem:\n",
    "        if any(word in opt for word in negatives):\n",
    "            print('--- Absolute word in question stem')\n",
    "            return False\n",
    "    \n",
    "    #Check for neatively worded stem too.\n",
    "    for sent in question.stem.split('.'):\n",
    "        sent = sent.lower()\n",
    "        \n",
    "        if 'which' in sent and ('false' in sent or 'not' in sent or 'incorrect' in sent or 'except' in sent) or \\\n",
    "        'what' in sent and ('false' in sent or 'not' in sent or 'incorrect' in sent or 'except' in sent):\n",
    "            return False    \n",
    "    \n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of how to use this rule-based method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The CSV should contain 5 columns in total, the question's text and answer choices\n",
    "    #question: question text\n",
    "    #altA, altB, altC, altD: answer choices\n",
    "qs = pd.read_csv('some_csv.csv')\n",
    "qs = qs.fillna('')\n",
    "\n",
    "questions = []\n",
    "for index, row in qs.iterrows():\n",
    "    question = MultipleChoiceQuestion(\n",
    "        stem=row['question'],\n",
    "        options=[row['altA'], row['altB'], row['altC'], row['altD']],\n",
    "        correct_option= row['altA']\n",
    "    )\n",
    "    questions.append(question)\n",
    "\n",
    "#Call each criteria on the question, save it to an array, which will become a row in our dataframe\n",
    "rows = []\n",
    "for q in questions:\n",
    "    r = [ambiguous_unclear_information(q),\n",
    "        implausible_distractors(q),\n",
    "        none_of_the_above(q),\n",
    "        longest_answer_correct(q),\n",
    "        gratuitous_information_in_stem(q),\n",
    "        true_or_false(q),\n",
    "        avoid_convergence_cues(q),\n",
    "        avoid_logical_cues(q),\n",
    "        all_of_the_above(q),\n",
    "        fill_in_the_blank(q),\n",
    "        absolute_terms(q),\n",
    "        word_repeats_in_stem_and_correct_answer(q),\n",
    "        unfocused_stem(q),\n",
    "        complex_k_type(q),\n",
    "        grammatical_cues_in_stem(q),\n",
    "        lost_sequence(q),\n",
    "        vague_terms(q),\n",
    "        more_than_one_correct(q),\n",
    "        negative_worded_stem(q)]\n",
    "    rows.append(r)\n",
    "\n",
    "#Set the columns up for our dataframe\n",
    "columns = [\n",
    "    'ambiguous_unclear_information',\n",
    "    'implausible_distractors',\n",
    "    'none_of_the_above',\n",
    "    'longest_answer_correct',\n",
    "    'gratuitous_information_in_stem',\n",
    "    'true_or_false',\n",
    "    'avoid_convergence_cues',\n",
    "    'avoid_logical_cues',\n",
    "    'all_of_the_above',\n",
    "    'fill_in_the_blank',\n",
    "    'absolute_terms',\n",
    "    'word_repeats_in_stem_and_correct_answer',\n",
    "    'unfocused_stem',\n",
    "    'complex_k_type',\n",
    "    'grammatical_cues_in_stem',\n",
    "    'lost_sequence',\n",
    "    'vague_terms',\n",
    "    'more_than_one_correct',\n",
    "    'negative_worded_stem'\n",
    "]\n",
    "\n",
    "results = pd.DataFrame(rows, columns=columns)\n",
    "results.to_csv(\"results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the IWF Rubric with GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "questions = data_structure_of_questions\n",
    "iwfs = [{\"criteria\": \"ambiguous or unclear information \",\n",
    "     \"definition\": \"questions and all options should be written in clear, unambiguous language\"}, \n",
    "     {\"criteria\": \"implausible distractors\",\n",
    "     \"definition\": \"questions should make all alternative answer choices plausible, as good items depend on having effective distractors\"}, \n",
    "     {\"criteria\": \"none of the above\",\n",
    "     \"definition\": \"questions should avoid using none of the above as an answer choice as it only really measures students ability to detect incorrect answers\"},\n",
    "     {\"criteria\": \"longest option is correct\",\n",
    "     \"definition\": \"all question options should be similar in length and the amount of detail provided in each option, however this can be ignored if the options are only a few words in length\"}, \n",
    "     {\"criteria\": \"gratuitous information in the stem\",\n",
    "     \"definition\": \"questions should avoid using gratuitous or unnecessary information in the stem that is not required to answer the question\"},\n",
    "    {\"criteria\": \"true/false question\",\n",
    "     \"definition\": \"question options should not be a series of true or false statements\"},\n",
    "    {\"criteria\": \"convergence cues\",\n",
    "     \"definition\": \"questions should avoid convergence cues in options where there are different combinations of multiple components to the answer\"}, \n",
    "    {\"criteria\": \"logical cues\",\n",
    "     \"definition\": \"questions should avoid clues in the stem and the correct option that can help the test-wise student to identify the correct option\"}, \n",
    "    {\"criteria\": \"all of the above\",\n",
    "     \"definition\": \"question options should not contain 'all of the above' or something similar, as students can guess correct responses based on partial information. If the question options do not contain it, then the question satisfies this criteria.\"},\n",
    "    {\"criteria\": \"fill-in-the-blank\",\n",
    "     \"definition\": \"questions should avoid omitting words in the middle of the stem that students must insert from the options provided\"}, \n",
    "    {\"criteria\": \"absolute terms\",\n",
    "     \"definition\": \"questions should avoid the use of absolute terms (e.g. never, always, only, all) in the options as students are aware that they are almost always false\"},\n",
    "    {\"criteria\": \"word repeats\",\n",
    "     \"definition\": \"questions should avoid repeating words between just the stem and the correct option, if there are repeated words, they should also be included among other options\"}, \n",
    "    {\"criteria\": \"unfocused stem\",\n",
    "     \"definition\": \"the question stem should present a clear and focused question that can be understood and answered without looking at the options\"},\n",
    "    {\"criteria\": \"complex or k-type\",\n",
    "     \"definition\": \"questions should avoid questions that have a range of correct responses, that ask students to select from a number of possible combinations of the responses\"}, \n",
    "    {\"criteria\": \"grammatical cues\",\n",
    "     \"definition\": \"all question options should be grammatically consistent with the stem and should be parallel in style and form\"}, \n",
    "    {\"criteria\": \"lost sequence\",\n",
    "     \"definition\": \"all question options should be arranged in chronological or numerical order\"},\n",
    "    {\"criteria\": \"vague terms\",\n",
    "     \"definition\": \"questions should avoid the use of vague terms (e.g. frequently, occasionally, rarely, usually, commonly) in the options, as these terms lack precision and there is seldom agreement on their actual meaning\"},\n",
    "     {\"criteria\": \"more than one correct\",\n",
    "     \"definition\": \"questions should only have one correct answer from the question options\"},\n",
    "     {\"criteria\": \"negative worded\",\n",
    "     \"definition\": \"questions should avoid the use of negative words (e.g., not, except, incorrect) in the stem\"}, \n",
    "    ]\n",
    "\n",
    "#Loop over each question, then for each question, call the IWF criteria one at a time on it.\n",
    "done = False\n",
    "for q in questions:\n",
    "    results.append(q.stem)\n",
    "    for i in iwfs:\n",
    "        #Run this as a while loop with error handling code, as sometimes the GPT-4 API goes down, returning an error, in which \n",
    "        #we'll need to wait and retry our call\n",
    "        while(done == False):\n",
    "            try:\n",
    "                o = openai.ChatCompletion.create(\n",
    "                  model=\"gpt-4\", \n",
    "                  messages=[\n",
    "                    {\"role\": \"user\", \"content\": f'Begin your response with yes or no, does this multiple-choice question satisfy the criteria relating to {i[\"criteria\"]}: {i[\"definition\"]}? Explain why. \\n' + q.stem + \" \\n\" + nl.join(q.options)},\n",
    "                  ]\n",
    "                 )\n",
    "                done = True \n",
    "            except:\n",
    "                time.sleep(150)\n",
    "        done = False\n",
    "        results.append(o)\n",
    "    \n",
    "    \n",
    "rows = []\n",
    "r = []\n",
    "indz = 0\n",
    "for res in results \n",
    "    try:\n",
    "        r.append(res.choices[0].message.content)\n",
    "    except:\n",
    "        r.append(res)\n",
    "        \n",
    "    #Once we've created a row, r, that contains the question text and 19 criteria, append it to our greater rows list\n",
    "    if indz == 19:\n",
    "        rows.append(r)\n",
    "        r = []\n",
    "        indz = 0\n",
    "    else:\n",
    "        indz = indz + 1\n",
    "\n",
    "\n",
    "columns = [\n",
    "    'question',\n",
    "    'ambiguous_unclear_information',\n",
    "    'implausible_distractors',\n",
    "    'none_of_the_above',\n",
    "    'longest_answer_correct',\n",
    "    'gratuitous_information_in_stem',\n",
    "    'true_or_false',\n",
    "    'avoid_convergence_cues',\n",
    "    'avoid_logical_cues',\n",
    "    'all_of_the_above',\n",
    "    'fill_in_the_blank',\n",
    "    'absolute_terms',\n",
    "    'word_repeats_in_stem_and_correct_answer',\n",
    "    'unfocused_stem',\n",
    "    'complex_k_type',\n",
    "    'grammatical_cues_in_stem',\n",
    "    'lost_sequence',\n",
    "    'vague_terms',\n",
    "    'more_than_one_correct',\n",
    "    'negative_worded_stem'\n",
    "]\n",
    "\n",
    "pd_results = pd.DataFrame(rows, columns=columns)\n",
    "pd_results.to_csv(\"gpt-4_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
